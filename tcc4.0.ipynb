{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM usando espectrogramas do SOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def calcula_lbp(imagem, P, R, metodo):\n",
    "    # Calcular o LBP\n",
    "    lbp = local_binary_pattern(imagem, P, R, metodo)\n",
    "    # Calcular o histograma LBP\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    hist, bins = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "    return hist\n",
    "\n",
    "# Abrir o arquivo para escrita\n",
    "arquivo_resultados = open('resultados.txt', 'w')\n",
    "\n",
    "# Parâmetros LBP\n",
    "P = 8  # Número de pontos vizinhos\n",
    "R = 2  # Raio\n",
    "metodo = 'nri_uniform'\n",
    "\n",
    "# Diretórios\n",
    "diretorios = ['base/fold1', 'base/fold2', 'base/fold3']\n",
    "\n",
    "hist = []\n",
    "tags = []\n",
    "\n",
    "# Carregar as imagens e calcular os histogramas LBP\n",
    "for i, diretorio in enumerate(diretorios):\n",
    "    lista_arquivos = os.listdir(diretorio)\n",
    "    for j, arquivo_img in enumerate(lista_arquivos):\n",
    "        caminho_imagem = os.path.join(diretorio, arquivo_img)\n",
    "        imagem = cv2.imread(caminho_imagem, cv2.IMREAD_GRAYSCALE)\n",
    "        hist.append(calcula_lbp(imagem, P, R, metodo))\n",
    "        tags.append(j // 30 + 1)  # Tag para cada imagem\n",
    "\n",
    "hist = np.array(hist)\n",
    "tags = np.array(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 79.22%\n",
      "Desvio padrão da acurácia: 1.66%\n",
      "Precisão média: 79.93%\n",
      "Recall médio: 79.22%\n",
      "F1-score médio: 79.05%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Função para realizar a validação cruzada\n",
    "def kfoldcv(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Configurar os parâmetros para o GridSearchCV (SVM)\n",
    "        param_grid = {\n",
    "            'C': [0.289],\n",
    "            'gamma': [1],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "\n",
    "        svm_model = SVC(random_state=42)\n",
    "        grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões e calcular as métricas\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        precisions.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "        recalls.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies) * 100  # Converter para porcentagem\n",
    "    std_accuracy = np.std(accuracies) * 100  # Converter para porcentagem\n",
    "    \n",
    "    mean_precision = np.mean(precisions) * 100\n",
    "    mean_recall = np.mean(recalls) * 100\n",
    "    mean_f1_score = np.mean(f1_scores) * 100\n",
    "\n",
    "    return mean_accuracy, std_accuracy, mean_precision, mean_recall, mean_f1_score\n",
    "\n",
    "# Realizar a validação cruzada\n",
    "mean_accuracy, std_accuracy, mean_precision, mean_recall, mean_f1_score = kfoldcv(hist, tags, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy:.2f}%')\n",
    "print(f'Desvio padrão da acurácia: {std_accuracy:.2f}%')\n",
    "print(f'Precisão média: {mean_precision:.2f}%')\n",
    "print(f'Recall médio: {mean_recall:.2f}%')\n",
    "print(f'F1-score médio: {mean_f1_score:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 65.56%\n",
      "Desvio padrão da acurácia: 2.28%\n",
      "Precisão média: 65.67%\n",
      "Desvio padrão da precisão: 3.08%\n",
      "Recall médio: 65.56%\n",
      "Desvio padrão do recall: 2.28%\n",
      "F1-score médio: 64.70%\n",
      "Desvio padrão do F1-score: 2.42%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Função para realizar a validação cruzada com KNN\n",
    "def kfoldcv_knn(hist, tags, k):\n",
    "    # Assumindo que os dados já estão ordenados conforme os folds nos diretórios\n",
    "    fold_size = len(hist) // k\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "\n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "\n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Definir a grade de parâmetros para o Grid Search do KNN\n",
    "        param_grid = {\n",
    "            'n_neighbors': [3, 5, 7, 9, 11, 13],\n",
    "            'metric': ['canberra', 'minkowski', 'euclidean', 'manhattan']\n",
    "        }\n",
    "\n",
    "        # Treinar o modelo KNN usando Grid Search\n",
    "        knn_model = KNeighborsClassifier()\n",
    "        grid_search = GridSearchCV(knn_model, param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_knn = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões e calcular as métricas\n",
    "        y_pred = best_knn.predict(X_test)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        precisions.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "        recalls.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "    mean_accuracy = np.mean(accuracies) * 100  # Converter para porcentagem\n",
    "    std_accuracy = np.std(accuracies) * 100  # Converter para porcentagem\n",
    "    \n",
    "    mean_precision = np.mean(precisions) * 100\n",
    "    std_precision = np.std(precisions) * 100\n",
    "    \n",
    "    mean_recall = np.mean(recalls) * 100\n",
    "    std_recall = np.std(recalls) * 100\n",
    "    \n",
    "    mean_f1_score = np.mean(f1_scores) * 100\n",
    "    std_f1_score = np.std(f1_scores) * 100\n",
    "\n",
    "    return mean_accuracy, std_accuracy, mean_precision, std_precision, mean_recall, std_recall, mean_f1_score, std_f1_score\n",
    "\n",
    "# Realizar a validação cruzada com KNN\n",
    "mean_accuracy, std_accuracy, mean_precision, std_precision, mean_recall, std_recall, mean_f1_score, std_f1_score = kfoldcv_knn(hist, tags, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy:.2f}%')\n",
    "print(f'Desvio padrão da acurácia: {std_accuracy:.2f}%')\n",
    "print(f'Precisão média: {mean_precision:.2f}%')\n",
    "print(f'Desvio padrão da precisão: {std_precision:.2f}%')\n",
    "print(f'Recall médio: {mean_recall:.2f}%')\n",
    "print(f'Desvio padrão do recall: {std_recall:.2f}%')\n",
    "print(f'F1-score médio: {mean_f1_score:.2f}%')\n",
    "print(f'Desvio padrão do F1-score: {std_f1_score:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 79.22% (Desvio padrão: 1.66%)\n",
      "Precisão média: 79.93% (Desvio padrão: 2.12%)\n",
      "Recall médio: 79.22% (Desvio padrão: 1.66%)\n",
      "F1-score médio: 79.05% (Desvio padrão: 1.74%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Função para realizar a validação cruzada\n",
    "def kfoldcv(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Configurar os parâmetros para o GridSearchCV (SVM)\n",
    "        param_grid = {\n",
    "            'C': [0.289],\n",
    "            'gamma': [1],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "\n",
    "        svm_model = SVC(random_state=42)\n",
    "        grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões e calcular as métricas\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        precisions.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "        recalls.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies) * 100  # Converter para porcentagem\n",
    "    std_accuracy = np.std(accuracies) * 100  # Converter para porcentagem\n",
    "    \n",
    "    mean_precision = np.mean(precisions) * 100\n",
    "    std_precision = np.std(precisions) * 100\n",
    "    \n",
    "    mean_recall = np.mean(recalls) * 100\n",
    "    std_recall = np.std(recalls) * 100\n",
    "    \n",
    "    mean_f1_score = np.mean(f1_scores) * 100\n",
    "    std_f1_score = np.std(f1_scores) * 100\n",
    "\n",
    "    return (mean_accuracy, std_accuracy, \n",
    "            mean_precision, std_precision, \n",
    "            mean_recall, std_recall, \n",
    "            mean_f1_score, std_f1_score)\n",
    "\n",
    "# Realizar a validação cruzada\n",
    "(mean_accuracy, std_accuracy, \n",
    " mean_precision, std_precision, \n",
    " mean_recall, std_recall, \n",
    " mean_f1_score, std_f1_score) = kfoldcv(hist, tags, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy:.2f}% (Desvio padrão: {std_accuracy:.2f}%)')\n",
    "print(f'Precisão média: {mean_precision:.2f}% (Desvio padrão: {std_precision:.2f}%)')\n",
    "print(f'Recall médio: {mean_recall:.2f}% (Desvio padrão: {std_recall:.2f}%)')\n",
    "print(f'F1-score médio: {mean_f1_score:.2f}% (Desvio padrão: {std_f1_score:.2f}%)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def calcula_lbp(imagem, P, R, metodo):\n",
    "    # Calcular o LBP\n",
    "    lbp = local_binary_pattern(imagem, P, R, metodo)\n",
    "    # Calcular o histograma LBP\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    hist, bins = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "    return hist\n",
    "\n",
    "# Parâmetros LBP\n",
    "P = 8  # Número de pontos vizinhos\n",
    "R = 2  # Raio\n",
    "metodo = 'nri_uniform'\n",
    "\n",
    "# Diretórios\n",
    "diretorios = ['base/fold1', 'base/fold2', 'base/fold3']\n",
    "\n",
    "hist = []\n",
    "tags = []\n",
    "\n",
    "# Carregar as imagens, dividir em 3 partes horizontais e calcular os histogramas LBP\n",
    "for i, diretorio in enumerate(diretorios):\n",
    "    lista_arquivos = os.listdir(diretorio)\n",
    "    for j, arquivo_img in enumerate(lista_arquivos):\n",
    "        caminho_imagem = os.path.join(diretorio, arquivo_img)\n",
    "        imagem = cv2.imread(caminho_imagem, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Dividir a imagem em 3 partes horizontais\n",
    "        altura, largura = imagem.shape\n",
    "        altura_dividida = altura // 3\n",
    "        \n",
    "        for k in range(3):  # 3 partes\n",
    "            parte_imagem = imagem[k*altura_dividida:(k+1)*altura_dividida, :]\n",
    "            hist.append(calcula_lbp(parte_imagem, P, R, metodo))\n",
    "            tags.append(j // 30 + 1)  # Tag para cada imagem\n",
    "            \n",
    "hist = np.array(hist)\n",
    "tags = np.array(tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1, ..., 10, 10, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 78.33\n",
      "Desvio padrão da acurácia: 1.52\n"
     ]
    }
   ],
   "source": [
    "# Função para realizar a validação cruzada\n",
    "def kfoldcv(hist, tags, k):\n",
    "    # Assumindo que os dados já estão ordenados conforme os folds nos diretórios\n",
    "    fold_size = len(hist) // k\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Treinar o modelo SVM\n",
    "        svm_model = SVC(kernel='linear', C=0.20, random_state=42)\n",
    "        svm_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Fazer previsões e calcular a acurácia\n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies) * 100  # Converter para porcentagem\n",
    "    std_accuracy = np.std(accuracies) * 100  # Converter para porcentagem\n",
    "\n",
    "    return mean_accuracy, std_accuracy\n",
    "\n",
    "# Realizar a validação cruzada\n",
    "mean_accuracy, std_accuracy = kfoldcv(hist, tags, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy:.2f}')\n",
    "print(f'Desvio padrão da acurácia: {std_accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m         caminho_imagem \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(diretorio, arquivo_img)\n\u001b[0;32m     34\u001b[0m         imagem \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(caminho_imagem, cv2\u001b[38;5;241m.\u001b[39mIMREAD_GRAYSCALE)\n\u001b[1;32m---> 35\u001b[0m         hist\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcalcula_lbp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimagem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetodo\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     36\u001b[0m         tags\u001b[38;5;241m.\u001b[39mappend(j \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m30\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Tag para cada imagem\u001b[39;00m\n\u001b[0;32m     38\u001b[0m hist \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(hist)\n",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m, in \u001b[0;36mcalcula_lbp\u001b[1;34m(imagem, P, R, metodo)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalcula_lbp\u001b[39m(imagem, P, R, metodo):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Calcular o LBP\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     lbp \u001b[38;5;241m=\u001b[39m \u001b[43mlocal_binary_pattern\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimagem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetodo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Calcular o histograma LBP\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     n_bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(lbp\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\skimage\\feature\\texture.py:359\u001b[0m, in \u001b[0;36mlocal_binary_pattern\u001b[1;34m(image, P, R, method)\u001b[0m\n\u001b[0;32m    353\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplying `local_binary_pattern` to floating-point images may \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    355\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgive unexpected results when small numerical differences between \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjacent pixels are present. It is recommended to use this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction with images of integer dtype.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    358\u001b[0m image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(image, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m--> 359\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43m_local_binary_pattern\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethods\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def calcula_lbp(imagem, P, R, metodo):\n",
    "    # Calcular o LBP\n",
    "    lbp = local_binary_pattern(imagem, P, R, metodo)\n",
    "    # Calcular o histograma LBP\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "    return hist\n",
    "\n",
    "# Parâmetros LBP\n",
    "P = 8  # Número de pontos vizinhos\n",
    "R = 2  # Raio\n",
    "metodo = 'nri_uniform'\n",
    "\n",
    "# Diretórios\n",
    "diretorios = ['base/fold1', 'base/fold2', 'base/fold3']\n",
    "\n",
    "hist = []\n",
    "tags = []\n",
    "\n",
    "# Carregar as imagens e calcular os histogramas LBP\n",
    "for i, diretorio in enumerate(diretorios):\n",
    "    lista_arquivos = os.listdir(diretorio)\n",
    "    for j, arquivo_img in enumerate(lista_arquivos):\n",
    "        caminho_imagem = os.path.join(diretorio, arquivo_img)\n",
    "        imagem = cv2.imread(caminho_imagem, cv2.IMREAD_GRAYSCALE)\n",
    "        hist.append(calcula_lbp(imagem, P, R, metodo))\n",
    "        tags.append(j // 30 + 1)  # Tag para cada imagem\n",
    "\n",
    "hist = np.array(hist)\n",
    "tags = np.array(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 79.22%\n",
      "Desvio padrão da acurácia: 1.66%\n"
     ]
    }
   ],
   "source": [
    "# Função para realizar a validação cruzada\n",
    "def kfoldcv(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Configurar os parâmetros para o GridSearchCV (SVM)\n",
    "        param_grid = {\n",
    "            'C': [0.289],\n",
    "            'gamma': [1],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "\n",
    "        svm_model = SVC(random_state=42)\n",
    "        grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões e calcular a acurácia\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies) * 100  # Converter para porcentagem\n",
    "    std_accuracy = np.std(accuracies) * 100  # Converter para porcentagem\n",
    "\n",
    "    return mean_accuracy, std_accuracy\n",
    "\n",
    "# Realizar a validação cruzada\n",
    "mean_accuracy, std_accuracy = kfoldcv(hist, tags, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy:.2f}%')\n",
    "print(f'Desvio padrão da acurácia: {std_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mean_accuracy, std_accuracy, mean_precision, mean_recall, mean_f1_score\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Realizar a validação cruzada\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m mean_accuracy, std_accuracy, mean_precision, mean_recall, mean_f1_score \u001b[38;5;241m=\u001b[39m kfoldcv(\u001b[43mhist\u001b[49m, tags, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAcurácia média: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDesvio padrão da acurácia: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hist' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Função para realizar a validação cruzada\n",
    "def kfoldcv(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Configurar os parâmetros para o GridSearchCV (SVM)\n",
    "        param_grid = {\n",
    "            'C': [0.289],\n",
    "            'gamma': [1],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "\n",
    "        svm_model = SVC(random_state=42)\n",
    "        grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões e calcular as métricas\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        precisions.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "        recalls.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies) * 100  # Converter para porcentagem\n",
    "    std_accuracy = np.std(accuracies) * 100  # Converter para porcentagem\n",
    "    \n",
    "    mean_precision = np.mean(precisions) * 100\n",
    "    mean_recall = np.mean(recalls) * 100\n",
    "    mean_f1_score = np.mean(f1_scores) * 100\n",
    "\n",
    "    return mean_accuracy, std_accuracy, mean_precision, mean_recall, mean_f1_score\n",
    "\n",
    "# Realizar a validação cruzada\n",
    "mean_accuracy, std_accuracy, mean_precision, mean_recall, mean_f1_score = kfoldcv(hist, tags, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy:.2f}%')\n",
    "print(f'Desvio padrão da acurácia: {std_accuracy:.2f}%')\n",
    "print(f'Precisão média: {mean_precision:.2f}%')\n",
    "print(f'Recall médio: {mean_recall:.2f}%')\n",
    "print(f'F1-score médio: {mean_f1_score:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 77.33%\n",
      "Desvio padrão da acurácia: 0.72%\n"
     ]
    }
   ],
   "source": [
    "# Função para realizar a validação cruzada\n",
    "def kfoldcv(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Configurar os parâmetros para o GridSearchCV (SVM)\n",
    "        param_grid = {\n",
    "            'C': [50, 10, 1, 0.1, 0.001, 0.0001],\n",
    "            'gamma': [100, 50, 10, 1, 0.1, 0.50, 0.001, 0.0001],\n",
    "            'kernel': ['rbf']\n",
    "        }\n",
    "\n",
    "        svm_model = SVC(random_state=42)\n",
    "        grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões e calcular a acurácia\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies) * 100  # Converter para porcentagem\n",
    "    std_accuracy = np.std(accuracies) * 100  # Converter para porcentagem\n",
    "\n",
    "    return mean_accuracy, std_accuracy\n",
    "\n",
    "# Realizar a validação cruzada\n",
    "mean_accuracy, std_accuracy = kfoldcv(hist, tags, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy:.2f}%')\n",
    "print(f'Desvio padrão da acurácia: {std_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest usando espectrogramas do SOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 70.22\n",
      "Desvio padrão da acurácia: 1.81\n"
     ]
    }
   ],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def calcula_lbp(imagem, P, R, metodo):\n",
    "    # Calcular o LBP\n",
    "    lbp = local_binary_pattern(imagem, P, R, metodo)\n",
    "    # Calcular o histograma LBP\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    hist, bins = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "    return hist\n",
    "\n",
    "\n",
    "# Parâmetros LBP\n",
    "P = 8  # Número de pontos vizinhos\n",
    "R = 2  # Raio\n",
    "metodo = 'nri_uniform'\n",
    "\n",
    "# Diretórios\n",
    "diretorios = ['base/fold1', 'base/fold2', 'base/fold3']\n",
    "\n",
    "hist = []\n",
    "tags = []\n",
    "\n",
    "# Carregar as imagens e calcular os histogramas LBP\n",
    "for i, diretorio in enumerate(diretorios):\n",
    "    lista_arquivos = os.listdir(diretorio)\n",
    "    for j, arquivo_img in enumerate(lista_arquivos):\n",
    "        caminho_imagem = os.path.join(diretorio, arquivo_img)\n",
    "        imagem = cv2.imread(caminho_imagem, cv2.IMREAD_GRAYSCALE)\n",
    "        hist.append(calcula_lbp(imagem, P, R, metodo))\n",
    "        tags.append(j // 30 + 1)  # Tag para cada imagem\n",
    "\n",
    "hist = np.array(hist)\n",
    "tags = np.array(tags)\n",
    "\n",
    "# Função para realizar a validação cruzada\n",
    "def kfoldcv(hist, tags, k):\n",
    "    # Assumindo que os dados já estão ordenados conforme os folds nos diretórios\n",
    "    fold_size = len(hist) // k\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Treinar o modelo SVM\n",
    "        rf_model = RandomForestClassifier(n_estimators=4000, random_state=0, n_jobs=-1)\n",
    "        rf_model.fit(X_train, y_train)\n",
    "         \n",
    "        # Fazer previsões e calcular a acurácia\n",
    "        y_pred = rf_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies) * 100  # Converter para porcentagem\n",
    "    std_accuracy = np.std(accuracies) * 100  # Converter para porcentagem\n",
    "\n",
    "    return mean_accuracy, std_accuracy\n",
    "\n",
    "# Realizar a validação cruzada\n",
    "mean_accuracy, std_accuracy = kfoldcv(hist, tags, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy:.2f}')\n",
    "print(f'Desvio padrão da acurácia: {std_accuracy:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest com grid search usando espectrogramas do SOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 68.67\n",
      "Desvio padrão da acurácia: 1.52\n"
     ]
    }
   ],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def calcula_lbp(imagem, P, R, metodo):\n",
    "    # Calcular o LBP\n",
    "    lbp = local_binary_pattern(imagem, P, R, metodo)\n",
    "    # Calcular o histograma LBP\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    hist, bins = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "    return hist\n",
    "\n",
    "\n",
    "# Parâmetros LBP\n",
    "P = 8  # Número de pontos vizinhos\n",
    "R = 2  # Raio\n",
    "metodo = 'nri_uniform'\n",
    "\n",
    "# Diretórios\n",
    "diretorios = ['base/fold1', 'base/fold2', 'base/fold3']\n",
    "\n",
    "hist = []\n",
    "tags = []\n",
    "\n",
    "# Carregar as imagens e calcular os histogramas LBP\n",
    "for i, diretorio in enumerate(diretorios):\n",
    "    lista_arquivos = os.listdir(diretorio)\n",
    "    for j, arquivo_img in enumerate(lista_arquivos):\n",
    "        caminho_imagem = os.path.join(diretorio, arquivo_img)\n",
    "        imagem = cv2.imread(caminho_imagem, cv2.IMREAD_GRAYSCALE)\n",
    "        hist.append(calcula_lbp(imagem, P, R, metodo))\n",
    "        tags.append(j // 30 + 1)  # Tag para cada imagem\n",
    "\n",
    "hist = np.array(hist)\n",
    "tags = np.array(tags)\n",
    "\n",
    "# Função para realizar a validação cruzada\n",
    "def kfoldcv(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        param_grid = {\n",
    "            'n_estimators': [200],\n",
    "            'max_depth': [None, 10, 20, 30, 40, 50]\n",
    "        }\n",
    "        rf_model = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "        grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies) * 100\n",
    "    std_accuracy = np.std(accuracies) * 100\n",
    "\n",
    "    return mean_accuracy, std_accuracy\n",
    "\n",
    "\n",
    "# Realizar a validação cruzada\n",
    "mean_accuracy, std_accuracy = kfoldcv(hist, tags, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy:.2f}')\n",
    "print(f'Desvio padrão da acurácia: {std_accuracy:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "knn usando espectrogramas do SOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 65.56%\n",
      "Desvio padrão da acurácia: 2.28%\n"
     ]
    }
   ],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def calcula_lbp(imagem, P, R, metodo):\n",
    "    # Calcular o LBP\n",
    "    lbp = local_binary_pattern(imagem, P, R, metodo)\n",
    "    # Calcular o histograma LBP\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    hist, bins = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "    return hist\n",
    "\n",
    "# Parâmetros LBP\n",
    "P = 8  # Número de pontos vizinhos\n",
    "R = 2  # Raio\n",
    "metodo = 'nri_uniform'\n",
    "\n",
    "# Diretórios\n",
    "diretorios = ['base/fold1', 'base/fold2', 'base/fold3']\n",
    "\n",
    "hist = []\n",
    "tags = []\n",
    "\n",
    "# Carregar as imagens e calcular os histogramas LBP\n",
    "for i, diretorio in enumerate(diretorios):\n",
    "    lista_arquivos = os.listdir(diretorio)\n",
    "    for j, arquivo in enumerate(lista_arquivos):\n",
    "        caminho_imagem = os.path.join(diretorio, arquivo)\n",
    "        imagem = cv2.imread(caminho_imagem, cv2.IMREAD_GRAYSCALE)\n",
    "        hist.append(calcula_lbp(imagem, P, R, metodo))\n",
    "        tags.append(j // 30 + 1)  # Tag para cada imagem\n",
    "\n",
    "hist = np.array(hist)\n",
    "tags = np.array(tags)\n",
    "\n",
    "# Função para realizar a validação cruzada com KNN\n",
    "def kfoldcv_knn(hist, tags, k):\n",
    "    # Assumindo que os dados já estão ordenados conforme os folds nos diretórios\n",
    "    fold_size = len(hist) // k\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "\n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "\n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Definir a grade de parâmetros para o Grid Search do knn\n",
    "        param_grid = {\n",
    "            'n_neighbors': [3, 5, 7, 9, 11, 13],\n",
    "            'metric': ['canberra', 'minkowski', 'euclidean', 'manhattan']\n",
    "        }\n",
    "\n",
    "\n",
    "        # Treinar o modelo KNN usando Grid Search\n",
    "        knn_model = KNeighborsClassifier()\n",
    "        grid_search = GridSearchCV(knn_model, param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_knn = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões e calcular a acurácia\n",
    "        y_pred = best_knn.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    mean_accuracy = np.mean(accuracies) * 100  # Converter para porcentagem\n",
    "    std_accuracy = np.std(accuracies) * 100  # Converter para porcentagem\n",
    "\n",
    "    return mean_accuracy, std_accuracy\n",
    "\n",
    "# Realizar a validação cruzada com KNN\n",
    "mean_accuracy, std_accuracy = kfoldcv_knn(hist, tags, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy:.2f}%')\n",
    "print(f'Desvio padrão da acurácia: {std_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 65.56%\n",
      "Desvio padrão da acurácia: 2.28%\n"
     ]
    }
   ],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def calcula_lbp(imagem, P, R, metodo):\n",
    "    # Calcular o LBP\n",
    "    lbp = local_binary_pattern(imagem, P, R, metodo)\n",
    "    # Calcular o histograma LBP\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    hist, bins = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "    return hist\n",
    "\n",
    "# Parâmetros LBP\n",
    "P = 8  # Número de pontos vizinhos\n",
    "R = 2  # Raio\n",
    "metodo = 'nri_uniform'\n",
    "\n",
    "# Diretórios\n",
    "diretorios = ['base/fold1', 'base/fold2', 'base/fold3']\n",
    "\n",
    "hist = []\n",
    "tags = []\n",
    "\n",
    "# Carregar as imagens e calcular os histogramas LBP\n",
    "for i, diretorio in enumerate(diretorios):\n",
    "    lista_arquivos = os.listdir(diretorio)\n",
    "    for j, arquivo in enumerate(lista_arquivos):\n",
    "        caminho_imagem = os.path.join(diretorio, arquivo)\n",
    "        imagem = cv2.imread(caminho_imagem, cv2.IMREAD_GRAYSCALE)\n",
    "        hist.append(calcula_lbp(imagem, P, R, metodo))\n",
    "        tags.append(j // 30 + 1)  # Tag para cada imagem\n",
    "\n",
    "hist = np.array(hist)\n",
    "tags = np.array(tags)\n",
    "\n",
    "# Função para realizar a validação cruzada com KNN\n",
    "def kfoldcv_knn(hist, tags, k):\n",
    "    # Assumindo que os dados já estão ordenados conforme os folds nos diretórios\n",
    "    fold_size = len(hist) // k\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "\n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "\n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Definir a grade de parâmetros para o Grid Search do knn\n",
    "        param_grid = {\n",
    "            'n_neighbors': [3, 5, 7, 9, 11, 13],\n",
    "            'metric': ['canberra', 'minkowski']\n",
    "        }\n",
    "\n",
    "\n",
    "        # Treinar o modelo KNN usando Grid Search\n",
    "        knn_model = KNeighborsClassifier()\n",
    "        grid_search = GridSearchCV(knn_model, param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_knn = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões e calcular a acurácia\n",
    "        y_pred = best_knn.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    mean_accuracy = np.mean(accuracies) * 100  # Converter para porcentagem\n",
    "    std_accuracy = np.std(accuracies) * 100  # Converter para porcentagem\n",
    "\n",
    "    return mean_accuracy, std_accuracy\n",
    "\n",
    "# Realizar a validação cruzada com KNN\n",
    "mean_accuracy, std_accuracy = kfoldcv_knn(hist, tags, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy:.2f}%')\n",
    "print(f'Desvio padrão da acurácia: {std_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM usando espectrogramas librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 73.00\n",
      "Desvio padrão da acurácia: 2.23\n"
     ]
    }
   ],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def calcula_lbp(imagem, P, R, metodo):\n",
    "    # Calcular o LBP\n",
    "    lbp = local_binary_pattern(imagem, P, R, metodo)\n",
    "    # Calcular o histograma LBP\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    hist, bins = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "    return hist\n",
    "\n",
    "# Abrir o arquivo para escrita\n",
    "arquivo_resultados = open('resultados.txt', 'w')\n",
    "\n",
    "# Parâmetros LBP\n",
    "P = 8  # Número de pontos vizinhos\n",
    "R = 2  # Raio\n",
    "metodo = 'nri_uniform'\n",
    "\n",
    "# Diretórios\n",
    "diretorios = ['espec/fold1', 'espec/fold2', 'espec/fold3']\n",
    "\n",
    "hist = []\n",
    "tags = []\n",
    "\n",
    "# Carregar as imagens e calcular os histogramas LBP\n",
    "for i, diretorio in enumerate(diretorios):\n",
    "    lista_arquivos = os.listdir(diretorio)\n",
    "    for j, arquivo_img in enumerate(lista_arquivos):\n",
    "        caminho_imagem = os.path.join(diretorio, arquivo_img)\n",
    "        imagem = cv2.imread(caminho_imagem, cv2.IMREAD_GRAYSCALE)\n",
    "        hist.append(calcula_lbp(imagem, P, R, metodo))\n",
    "        tags.append(j // 30 + 1)  # Tag para cada imagem\n",
    "\n",
    "hist = np.array(hist)\n",
    "tags = np.array(tags)\n",
    "\n",
    "# Função para realizar a validação cruzada\n",
    "def kfoldcv(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Configurar os parâmetros para o GridSearchCV (SVM)\n",
    "        param_grid = {\n",
    "            'C': [50, 10, 1, 0.1, 0.001, 0.0001],\n",
    "            'gamma': [100, 50, 10, 1, 0.1, 0.50, 0.001, 0.0001],\n",
    "            'kernel': ['rbf']\n",
    "        }\n",
    "\n",
    "        svm_model = SVC(random_state=42)\n",
    "        grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões e calcular a acurácia\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies) * 100  # Converter para porcentagem\n",
    "    std_accuracy = np.std(accuracies) * 100  # Converter para porcentagem\n",
    "\n",
    "    return mean_accuracy, std_accuracy\n",
    "\n",
    "# Realizar a validação cruzada\n",
    "mean_accuracy, std_accuracy = kfoldcv(hist, tags, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy:.2f}')\n",
    "print(f'Desvio padrão da acurácia: {std_accuracy:.2f}')\n",
    "\n",
    "# Fechar o arquivo após terminar\n",
    "arquivo_resultados.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 70.89%\n",
      "Desvio padrão da acurácia: 2.74%\n"
     ]
    }
   ],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def calcula_lbp(imagem, P, R, metodo):\n",
    "    # Calcular o LBP\n",
    "    lbp = local_binary_pattern(imagem, P, R, metodo)\n",
    "    # Calcular o histograma LBP\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    hist, bins = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "    return hist\n",
    "\n",
    "# Abrir o arquivo para escrita\n",
    "arquivo_resultados = open('resultados.txt', 'w')\n",
    "\n",
    "# Parâmetros LBP\n",
    "P = 8  # Número de pontos vizinhos\n",
    "R = 2  # Raio\n",
    "metodo = 'nri_uniform'\n",
    "\n",
    "# Diretórios\n",
    "diretorios = ['espec/fold1', 'espec/fold2', 'espec/fold3']\n",
    "\n",
    "hist = []\n",
    "tags = []\n",
    "\n",
    "# Carregar as imagens e calcular os histogramas LBP\n",
    "for i, diretorio in enumerate(diretorios):\n",
    "    lista_arquivos = os.listdir(diretorio)\n",
    "    for j, arquivo_img in enumerate(lista_arquivos):\n",
    "        caminho_imagem = os.path.join(diretorio, arquivo_img)\n",
    "        imagem = cv2.imread(caminho_imagem, cv2.IMREAD_GRAYSCALE)\n",
    "        hist.append(calcula_lbp(imagem, P, R, metodo))\n",
    "        tags.append(j // 30 + 1)  # Tag para cada imagem\n",
    "\n",
    "hist = np.array(hist)\n",
    "tags = np.array(tags)\n",
    "\n",
    "# Função para realizar a validação cruzada\n",
    "def kfoldcv(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Configurar os parâmetros para o GridSearchCV (SVM)\n",
    "        param_grid = {\n",
    "            'C': [0.289],\n",
    "            'gamma': [1],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "\n",
    "        svm_model = SVC(random_state=42)\n",
    "        grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões e calcular a acurácia\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies) * 100  # Converter para porcentagem\n",
    "    std_accuracy = np.std(accuracies) * 100  # Converter para porcentagem\n",
    "\n",
    "    return mean_accuracy, std_accuracy\n",
    "\n",
    "# Realizar a validação cruzada\n",
    "mean_accuracy, std_accuracy = kfoldcv(hist, tags, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy:.2f}%')\n",
    "print(f'Desvio padrão da acurácia: {std_accuracy:.2f}%')\n",
    "\n",
    "# Fechar o arquivo após terminar\n",
    "arquivo_resultados.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest usando librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 65.56\n",
      "Desvio padrão da acurácia: 3.40\n"
     ]
    }
   ],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def calcula_lbp(imagem, P, R, metodo):\n",
    "    # Calcular o LBP\n",
    "    lbp = local_binary_pattern(imagem, P, R, metodo)\n",
    "    # Calcular o histograma LBP\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    hist, bins = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "    return hist\n",
    "\n",
    "\n",
    "# Parâmetros LBP\n",
    "P = 8  # Número de pontos vizinhos\n",
    "R = 2  # Raio\n",
    "metodo = 'nri_uniform'\n",
    "\n",
    "# Diretórios\n",
    "diretorios = ['espec/fold1', 'espec/fold2', 'espec/fold3']\n",
    "\n",
    "hist = []\n",
    "tags = []\n",
    "\n",
    "# Carregar as imagens e calcular os histogramas LBP\n",
    "for i, diretorio in enumerate(diretorios):\n",
    "    lista_arquivos = os.listdir(diretorio)\n",
    "    for j, arquivo_img in enumerate(lista_arquivos):\n",
    "        caminho_imagem = os.path.join(diretorio, arquivo_img)\n",
    "        imagem = cv2.imread(caminho_imagem, cv2.IMREAD_GRAYSCALE)\n",
    "        hist.append(calcula_lbp(imagem, P, R, metodo))\n",
    "        tags.append(j // 30 + 1)  # Tag para cada imagem\n",
    "\n",
    "hist = np.array(hist)\n",
    "tags = np.array(tags)\n",
    "\n",
    "# Função para realizar a validação cruzada\n",
    "def kfoldcv(hist, tags, k):\n",
    "    # Assumindo que os dados já estão ordenados conforme os folds nos diretórios\n",
    "    fold_size = len(hist) // k\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Treinar o modelo SVM\n",
    "        rf_model = RandomForestClassifier(n_estimators=4000, random_state=0, n_jobs=-1)\n",
    "        rf_model.fit(X_train, y_train)\n",
    "         \n",
    "        # Fazer previsões e calcular a acurácia\n",
    "        y_pred = rf_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies) * 100  # Converter para porcentagem\n",
    "    std_accuracy = np.std(accuracies) * 100  # Converter para porcentagem\n",
    "\n",
    "    return mean_accuracy, std_accuracy\n",
    "\n",
    "# Realizar a validação cruzada\n",
    "mean_accuracy, std_accuracy = kfoldcv(hist, tags, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy:.2f}')\n",
    "print(f'Desvio padrão da acurácia: {std_accuracy:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "knn usando librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 64.11%\n",
      "Desvio padrão da acurácia: 1.23%\n"
     ]
    }
   ],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def calcula_lbp(imagem, P, R, metodo):\n",
    "    # Calcular o LBP\n",
    "    lbp = local_binary_pattern(imagem, P, R, metodo)\n",
    "    # Calcular o histograma LBP\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    hist, bins = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "    return hist\n",
    "\n",
    "# Parâmetros LBP\n",
    "P = 8  # Número de pontos vizinhos\n",
    "R = 2  # Raio\n",
    "metodo = 'nri_uniform'\n",
    "\n",
    "# Diretórios\n",
    "diretorios = ['espec/fold1', 'espec/fold2', 'espec/fold3']\n",
    "\n",
    "hist = []\n",
    "tags = []\n",
    "\n",
    "# Carregar as imagens e calcular os histogramas LBP\n",
    "for i, diretorio in enumerate(diretorios):\n",
    "    lista_arquivos = os.listdir(diretorio)\n",
    "    for j, arquivo in enumerate(lista_arquivos):\n",
    "        caminho_imagem = os.path.join(diretorio, arquivo)\n",
    "        imagem = cv2.imread(caminho_imagem, cv2.IMREAD_GRAYSCALE)\n",
    "        hist.append(calcula_lbp(imagem, P, R, metodo))\n",
    "        tags.append(j // 30 + 1)  # Tag para cada imagem\n",
    "\n",
    "hist = np.array(hist)\n",
    "tags = np.array(tags)\n",
    "\n",
    "# Função para realizar a validação cruzada com KNN\n",
    "def kfoldcv_knn(hist, tags, k):\n",
    "    # Assumindo que os dados já estão ordenados conforme os folds nos diretórios\n",
    "    fold_size = len(hist) // k\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "\n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "\n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Definir a grade de parâmetros para o Grid Search\n",
    "        param_grid = {'n_neighbors': np.arange(1, 21)}\n",
    "\n",
    "        # Treinar o modelo KNN usando Grid Search\n",
    "        knn_model = KNeighborsClassifier()\n",
    "        grid_search = GridSearchCV(knn_model, param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_knn = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões e calcular a acurácia\n",
    "        y_pred = best_knn.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    mean_accuracy = np.mean(accuracies) * 100  # Converter para porcentagem\n",
    "    std_accuracy = np.std(accuracies) * 100  # Converter para porcentagem\n",
    "\n",
    "    return mean_accuracy, std_accuracy\n",
    "\n",
    "# Realizar a validação cruzada com KNN\n",
    "mean_accuracy, std_accuracy = kfoldcv_knn(hist, tags, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy:.2f}%')\n",
    "print(f'Desvio padrão da acurácia: {std_accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
