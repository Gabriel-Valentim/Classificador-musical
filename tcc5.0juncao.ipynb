{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def calcula_lbp(imagem, P, R, metodo):\n",
    "    # Calcular o LBP\n",
    "    lbp = local_binary_pattern(imagem, P, R, metodo)\n",
    "    # Calcular o histograma LBP\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    hist, bins = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "    return hist\n",
    "\n",
    "# Abrir o arquivo para escrita\n",
    "arquivo_resultados = open('resultados.txt', 'w')\n",
    "\n",
    "# Parâmetros LBP\n",
    "P = 8  # Número de pontos vizinhos\n",
    "R = 2  # Raio\n",
    "metodo = 'nri_uniform'\n",
    "\n",
    "# Diretórios\n",
    "diretorios = ['base/fold1', 'base/fold2', 'base/fold3']\n",
    "\n",
    "hist = []\n",
    "tags = []\n",
    "\n",
    "# Carregar as imagens e calcular os histogramas LBP\n",
    "for i, diretorio in enumerate(diretorios):\n",
    "    lista_arquivos = os.listdir(diretorio)\n",
    "    for j, arquivo_img in enumerate(lista_arquivos):\n",
    "        caminho_imagem = os.path.join(diretorio, arquivo_img)\n",
    "        imagem = cv2.imread(caminho_imagem, cv2.IMREAD_GRAYSCALE)\n",
    "        hist.append(calcula_lbp(imagem, P, R, metodo))\n",
    "        tags.append(j // 30 + 1)  # Tag para cada imagem\n",
    "\n",
    "hist_ori = np.array(hist)\n",
    "tags_ori = np.array(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def calcula_lbp(imagem, P, R, metodo):\n",
    "    # Calcular o LBP\n",
    "    lbp = local_binary_pattern(imagem, P, R, metodo)\n",
    "    # Calcular o histograma LBP\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    hist, bins = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "    return hist\n",
    "\n",
    "# Abrir o arquivo para escrita\n",
    "arquivo_resultados = open('resultados.txt', 'w')\n",
    "\n",
    "# Parâmetros LBP\n",
    "P = 8  # Número de pontos vizinhos\n",
    "R = 2  # Raio\n",
    "metodo = 'nri_uniform'\n",
    "\n",
    "# Diretórios\n",
    "diretorios = ['harmonico/fold1', 'harmonico/fold2', 'harmonico/fold3']\n",
    "\n",
    "hist = []\n",
    "tags = []\n",
    "\n",
    "# Carregar as imagens e calcular os histogramas LBP\n",
    "for i, diretorio in enumerate(diretorios):\n",
    "    lista_arquivos = os.listdir(diretorio)\n",
    "    for j, arquivo_img in enumerate(lista_arquivos):\n",
    "        caminho_imagem = os.path.join(diretorio, arquivo_img)\n",
    "        imagem = cv2.imread(caminho_imagem, cv2.IMREAD_GRAYSCALE)\n",
    "        hist.append(calcula_lbp(imagem, P, R, metodo))\n",
    "        tags.append(j // 30 + 1)  # Tag para cada imagem\n",
    "\n",
    "hist_har = np.array(hist)\n",
    "tags_har = np.array(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def calcula_lbp(imagem, P, R, metodo):\n",
    "    # Calcular o LBP\n",
    "    lbp = local_binary_pattern(imagem, P, R, metodo)\n",
    "    # Calcular o histograma LBP\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    hist, bins = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "    return hist\n",
    "\n",
    "# Abrir o arquivo para escrita\n",
    "arquivo_resultados = open('resultados.txt', 'w')\n",
    "\n",
    "# Parâmetros LBP\n",
    "P = 8  # Número de pontos vizinhos\n",
    "R = 2  # Raio\n",
    "metodo = 'nri_uniform'\n",
    "\n",
    "# Diretórios\n",
    "diretorios = ['percussivo/fold1', 'percussivo/fold2', 'percussivo/fold3']\n",
    "\n",
    "hist = []\n",
    "tags = []\n",
    "\n",
    "# Carregar as imagens e calcular os histogramas LBP\n",
    "for i, diretorio in enumerate(diretorios):\n",
    "    lista_arquivos = os.listdir(diretorio)\n",
    "    for j, arquivo_img in enumerate(lista_arquivos):\n",
    "        caminho_imagem = os.path.join(diretorio, arquivo_img)\n",
    "        imagem = cv2.imread(caminho_imagem, cv2.IMREAD_GRAYSCALE)\n",
    "        hist.append(calcula_lbp(imagem, P, R, metodo))\n",
    "        tags.append(j // 30 + 1)  # Tag para cada imagem\n",
    "\n",
    "hist_per = np.array(hist)\n",
    "tags_per = np.array(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 79.22%\n",
      "Desvio padrão da acurácia: 1.66%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Função para realizar a validação cruzada\n",
    "def kfoldcv(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Configurar os parâmetros para o GridSearchCV (SVM)\n",
    "        param_grid = {\n",
    "            'C': [0.289],\n",
    "            'gamma': [1],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "\n",
    "        svm_model = SVC(random_state=42)\n",
    "        grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões e calcular a acurácia\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    mean_accuracy_ori = np.mean(accuracies) * 100  # Converter para porcentagem\n",
    "    std_accuracy_ori = np.std(accuracies) * 100  # Converter para porcentagem\n",
    "\n",
    "    return mean_accuracy_ori, std_accuracy_ori\n",
    "\n",
    "# Realizar a validação cruzada\n",
    "mean_accuracy_ori, std_accuracy_ori = kfoldcv(hist_ori, tags_ori, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy_ori:.2f}%')\n",
    "print(f'Desvio padrão da acurácia: {std_accuracy_ori:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 74.44%\n",
      "Desvio padrão da acurácia: 3.50%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Função para realizar a validação cruzada\n",
    "def kfoldcv(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Configurar os parâmetros para o GridSearchCV (SVM)\n",
    "        param_grid = {\n",
    "            'C': [0.289],\n",
    "            'gamma': [1],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "\n",
    "        svm_model = SVC(random_state=42)\n",
    "        grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões e calcular a acurácia\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    mean_accuracy_har = np.mean(accuracies) * 100  # Converter para porcentagem\n",
    "    std_accuracy_har = np.std(accuracies) * 100  # Converter para porcentagem\n",
    "\n",
    "    return mean_accuracy_har, std_accuracy_har\n",
    "\n",
    "# Realizar a validação cruzada\n",
    "mean_accuracy_har, std_accuracy_har = kfoldcv(hist_har, tags_har, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy_har:.2f}%')\n",
    "print(f'Desvio padrão da acurácia: {std_accuracy_har:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 75.33%\n",
      "Desvio padrão da acurácia: 2.37%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Função para realizar a validação cruzada\n",
    "def kfoldcv(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Configurar os parâmetros para o GridSearchCV (SVM)\n",
    "        param_grid = {\n",
    "            'C': [0.289],\n",
    "            'gamma': [1],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "\n",
    "        svm_model = SVC(random_state=42)\n",
    "        grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões e calcular a acurácia\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies) * 100  # Converter para porcentagem\n",
    "    std_accuracy = np.std(accuracies) * 100  # Converter para porcentagem\n",
    "\n",
    "    return mean_accuracy, std_accuracy\n",
    "\n",
    "# Realizar a validação cruzada\n",
    "mean_accuracy_per, std_accuracy_per = kfoldcv(hist_per, tags_per, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy_per:.2f}%')\n",
    "print(f'Desvio padrão da acurácia: {std_accuracy_per:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 73.44%\n",
      "Desvio padrão da acurácia: 0.31%\n"
     ]
    }
   ],
   "source": [
    "# Função para realizar a validação cruzada\n",
    "def kfoldcv(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Configurar os parâmetros para o GridSearchCV (SVM)\n",
    "        param_grid = {\n",
    "            'C': [50, 10, 1, 0.1, 0.001, 0.0001],\n",
    "            'gamma': [100, 50, 10, 1, 0.1, 0.50, 0.001, 0.0001],\n",
    "            'kernel': ['rbf']\n",
    "        }\n",
    "\n",
    "        svm_model = SVC(random_state=42)\n",
    "        grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões e calcular a acurácia\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies) * 100  # Converter para porcentagem\n",
    "    std_accuracy = np.std(accuracies) * 100  # Converter para porcentagem\n",
    "\n",
    "    return mean_accuracy, std_accuracy\n",
    "\n",
    "# Realizar a validação cruzada\n",
    "mean_accuracy, std_accuracy = kfoldcv(hist, tags, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy:.2f}%')\n",
    "print(f'Desvio padrão da acurácia: {std_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 66.00\n",
      "Desvio padrão da acurácia: 1.89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Função para realizar a validação cruzada\n",
    "def kfoldcv(hist, tags, k):\n",
    "    # Assumindo que os dados já estão ordenados conforme os folds nos diretórios\n",
    "    fold_size = len(hist) // k\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Treinar o modelo SVM\n",
    "        rf_model = RandomForestClassifier(n_estimators=4000, random_state=0, n_jobs=-1)\n",
    "        rf_model.fit(X_train, y_train)\n",
    "         \n",
    "        # Fazer previsões e calcular a acurácia\n",
    "        y_pred = rf_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies) * 100  # Converter para porcentagem\n",
    "    std_accuracy = np.std(accuracies) * 100  # Converter para porcentagem\n",
    "\n",
    "    return mean_accuracy, std_accuracy\n",
    "\n",
    "# Realizar a validação cruzada\n",
    "mean_accuracy, std_accuracy = kfoldcv(hist, tags, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy:.2f}')\n",
    "print(f'Desvio padrão da acurácia: {std_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 66.11\n",
      "Desvio padrão da acurácia: 1.97\n"
     ]
    }
   ],
   "source": [
    "# Função para realizar a validação cruzada\n",
    "def kfoldcv(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        param_grid = {\n",
    "            'n_estimators': [200, 2000, 4000],\n",
    "            'max_depth': [None, 10, 20, 30, 40, 50]\n",
    "        }\n",
    "        rf_model = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "        grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies) * 100\n",
    "    std_accuracy = np.std(accuracies) * 100\n",
    "\n",
    "    return mean_accuracy, std_accuracy\n",
    "\n",
    "\n",
    "# Realizar a validação cruzada\n",
    "mean_accuracy, std_accuracy = kfoldcv(hist, tags, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy:.2f}')\n",
    "print(f'Desvio padrão da acurácia: {std_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia média: 64.33%\n",
      "Desvio padrão da acurácia: 0.72%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Função para realizar a validação cruzada com KNN\n",
    "def kfoldcv_knn(hist, tags, k):\n",
    "    # Assumindo que os dados já estão ordenados conforme os folds nos diretórios\n",
    "    fold_size = len(hist) // k\n",
    "    accuracies = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "\n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "\n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Definir a grade de parâmetros para o Grid Search\n",
    "        param_grid = {'n_neighbors': np.arange(1, 21)}\n",
    "\n",
    "        # Treinar o modelo KNN usando Grid Search\n",
    "        knn_model = KNeighborsClassifier()\n",
    "        grid_search = GridSearchCV(knn_model, param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_knn = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões e calcular a acurácia\n",
    "        y_pred = best_knn.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    mean_accuracy = np.mean(accuracies) * 100  # Converter para porcentagem\n",
    "    std_accuracy = np.std(accuracies) * 100  # Converter para porcentagem\n",
    "\n",
    "    return mean_accuracy, std_accuracy\n",
    "\n",
    "# Realizar a validação cruzada com KNN\n",
    "mean_accuracy, std_accuracy = kfoldcv_knn(hist, tags, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy:.2f}%')\n",
    "print(f'Desvio padrão da acurácia: {std_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia (regra da soma): 81.00%\n",
      "Acurácia (regra do produto): 81.56%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Função para realizar predições com probabilidades\n",
    "def kfoldcv_proba(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    all_probas = []\n",
    "    y_tests = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Configurar os parâmetros para o GridSearchCV (SVM)\n",
    "        param_grid = {\n",
    "            'C': [0.289],\n",
    "            'gamma': [1],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "\n",
    "        svm_model = SVC(random_state=42, probability=True)\n",
    "        grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões de probabilidades\n",
    "        y_proba = best_model.predict_proba(X_test)\n",
    "        all_probas.append(y_proba)\n",
    "        y_tests.append(y_test)\n",
    "    \n",
    "    return np.concatenate(all_probas), np.concatenate(y_tests)\n",
    "\n",
    "# Realizar a validação cruzada com probabilidades para cada conjunto de dados\n",
    "proba_ori, y_test = kfoldcv_proba(hist_ori, tags_ori, 3)\n",
    "proba_har, _ = kfoldcv_proba(hist_har, tags_har, 3)\n",
    "proba_per, _ = kfoldcv_proba(hist_per, tags_per, 3)\n",
    "\n",
    "# Fusão das predições usando a regra da soma\n",
    "fused_proba_soma = (proba_ori + proba_har + proba_per) / 3\n",
    "fused_pred_soma = np.argmax(fused_proba_soma, axis=1)\n",
    "\n",
    "for i in range(0, len(fused_pred_soma)):\n",
    "    fused_pred_soma[i] = fused_pred_soma[i] + 1\n",
    "\n",
    "# Fusão das predições usando a regra do produto\n",
    "fused_proba_produto = proba_ori * proba_har * proba_per\n",
    "fused_pred_produto = np.argmax(fused_proba_produto, axis=1)\n",
    "\n",
    "for i in range(0, len(fused_pred_produto)):\n",
    "    fused_pred_produto[i] = fused_pred_produto[i] + 1\n",
    "\n",
    "# Avaliação das fusões\n",
    "accuracy_soma = accuracy_score(y_test, fused_pred_soma)\n",
    "accuracy_produto = accuracy_score(y_test, fused_pred_produto)\n",
    "\n",
    "print(f'Acurácia (regra da soma): {accuracy_soma * 100:.2f}%')\n",
    "print(f'Acurácia (regra do produto): {accuracy_produto * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia (regra da soma): 71.44%\n",
      "Acurácia (regra do produto): 72.44%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Função para realizar predições com probabilidades usando Random Forest\n",
    "def kfoldcv_proba_rf(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    all_probas = []\n",
    "    y_tests = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Configurar os parâmetros para o GridSearchCV (Random Forest)\n",
    "        param_grid = {\n",
    "            'n_estimators': [1000, 2000, 4000],\n",
    "            'max_features': ['sqrt', 'log2'],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'random_state': [42]\n",
    "        }\n",
    "\n",
    "        rf_model = RandomForestClassifier(n_jobs=-1)\n",
    "        grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões de probabilidades\n",
    "        y_proba = best_model.predict_proba(X_test)\n",
    "        all_probas.append(y_proba)\n",
    "        y_tests.append(y_test)\n",
    "    \n",
    "    return np.concatenate(all_probas), np.concatenate(y_tests)\n",
    "\n",
    "# Realizar a validação cruzada com probabilidades para cada conjunto de dados\n",
    "proba_ori, y_test = kfoldcv_proba_rf(hist_ori, tags_ori, 3)\n",
    "proba_har, _ = kfoldcv_proba_rf(hist_har, tags_har, 3)\n",
    "proba_per, _ = kfoldcv_proba_rf(hist_per, tags_per, 3)\n",
    "\n",
    "# Fusão das predições usando a regra da soma\n",
    "fused_proba_soma = (proba_ori + proba_har + proba_per) / 3\n",
    "fused_pred_soma = np.argmax(fused_proba_soma, axis=1)\n",
    "\n",
    "# Incrementar as predições para começar de 1, conforme necessário\n",
    "fused_pred_soma = fused_pred_soma + 1\n",
    "\n",
    "# Fusão das predições usando a regra do produto\n",
    "fused_proba_produto = proba_ori * proba_har * proba_per\n",
    "fused_pred_produto = np.argmax(fused_proba_produto, axis=1)\n",
    "\n",
    "# Incrementar as predições para começar de 1, conforme necessário\n",
    "fused_pred_produto = fused_pred_produto + 1\n",
    "\n",
    "# Avaliação das fusões\n",
    "accuracy_soma = accuracy_score(y_test, fused_pred_soma)\n",
    "accuracy_produto = accuracy_score(y_test, fused_pred_produto)\n",
    "\n",
    "print(f'Acurácia (regra da soma): {accuracy_soma * 100:.2f}%')\n",
    "print(f'Acurácia (regra do produto): {accuracy_produto * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regra da Soma - Acurácia: 81.00%\n",
      "Regra da Soma - Precisão: 81.20%\n",
      "Regra da Soma - Recall: 81.00%\n",
      "Regra da Soma - F1-score: 80.99%\n",
      "----------------------------------------------------\n",
      "Regra do Produto - Acurácia: 81.56%\n",
      "Regra do Produto - Precisão: 81.79%\n",
      "Regra do Produto - Recall: 81.56%\n",
      "Regra do Produto - F1-score: 81.56%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Função para realizar predições com probabilidades\n",
    "def kfoldcv_proba(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    all_probas = []\n",
    "    y_tests = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Configurar os parâmetros para o GridSearchCV (SVM)\n",
    "        param_grid = {\n",
    "            'C': [0.289],\n",
    "            'gamma': [1],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "\n",
    "        svm_model = SVC(random_state=42, probability=True)\n",
    "        grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões de probabilidades\n",
    "        y_proba = best_model.predict_proba(X_test)\n",
    "        all_probas.append(y_proba)\n",
    "        y_tests.append(y_test)\n",
    "    \n",
    "    return np.concatenate(all_probas), np.concatenate(y_tests)\n",
    "\n",
    "# Realizar a validação cruzada com probabilidades para cada conjunto de dados\n",
    "proba_ori_svm, y_test = kfoldcv_proba(hist_ori, tags_ori, 3)\n",
    "proba_har_svm, _ = kfoldcv_proba(hist_har, tags_har, 3)\n",
    "proba_per_svm, _ = kfoldcv_proba(hist_per, tags_per, 3)\n",
    "\n",
    "# Fusão das predições usando a regra da soma\n",
    "fused_proba_soma = (proba_ori_svm + proba_har_svm + proba_per_svm) / 3\n",
    "fused_pred_soma = np.argmax(fused_proba_soma, axis=1)\n",
    "\n",
    "for i in range(0, len(fused_pred_soma)):\n",
    "    fused_pred_soma[i] = fused_pred_soma[i] + 1\n",
    "\n",
    "# Fusão das predições usando a regra do produto\n",
    "fused_proba_produto = proba_ori_svm * proba_har_svm * proba_per_svm\n",
    "fused_pred_produto = np.argmax(fused_proba_produto, axis=1)\n",
    "\n",
    "for i in range(0, len(fused_pred_produto)):\n",
    "    fused_pred_produto[i] = fused_pred_produto[i] + 1\n",
    "\n",
    "# Avaliação das fusões\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred, method_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    precision = precision_score(y_true, y_pred, average='weighted') * 100\n",
    "    recall = recall_score(y_true, y_pred, average='weighted') * 100\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted') * 100\n",
    "\n",
    "    print(f'{method_name} - Acurácia: {accuracy:.2f}%')\n",
    "    print(f'{method_name} - Precisão: {precision:.2f}%')\n",
    "    print(f'{method_name} - Recall: {recall:.2f}%')\n",
    "    print(f'{method_name} - F1-score: {f1:.2f}%')\n",
    "\n",
    "# Avaliar as predições usando a regra da soma\n",
    "evaluate_predictions(y_test, fused_pred_soma, \"Regra da Soma\")\n",
    "\n",
    "print('----------------------------------------------------')\n",
    "\n",
    "# Avaliar as predições usando a regra do produto\n",
    "evaluate_predictions(y_test, fused_pred_produto, \"Regra do Produto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regra da Soma - Acurácia: 81.00% ± 0.00%\n",
      "Regra da Soma - Precisão: 81.20% ± 0.00%\n",
      "Regra da Soma - Recall: 81.00% ± 0.00%\n",
      "Regra da Soma - F1-score: 80.99% ± 0.00%\n",
      "\n",
      "----------------------------------------------------\n",
      "Regra do Produto - Acurácia: 81.56% ± 0.00%\n",
      "Regra do Produto - Precisão: 81.79% ± 0.00%\n",
      "Regra do Produto - Recall: 81.56% ± 0.00%\n",
      "Regra do Produto - F1-score: 81.56% ± 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Função para realizar predições com probabilidades\n",
    "def kfoldcv_proba(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    all_probas = []\n",
    "    y_tests = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Configurar os parâmetros para o GridSearchCV (SVM)\n",
    "        param_grid = {\n",
    "            'C': [0.289],\n",
    "            'gamma': [1],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "\n",
    "        svm_model = SVC(random_state=42, probability=True)\n",
    "        grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões de probabilidades\n",
    "        y_proba = best_model.predict_proba(X_test)\n",
    "        all_probas.append(y_proba)\n",
    "        y_tests.append(y_test)\n",
    "    \n",
    "    return np.concatenate(all_probas), np.concatenate(y_tests)\n",
    "\n",
    "# Realizar a validação cruzada com probabilidades para cada conjunto de dados\n",
    "proba_ori_svm, y_test = kfoldcv_proba(hist_ori, tags_ori, 3)\n",
    "proba_har_svm, _ = kfoldcv_proba(hist_har, tags_har, 3)\n",
    "proba_per_svm, _ = kfoldcv_proba(hist_per, tags_per, 3)\n",
    "\n",
    "# Fusão das predições usando a regra da soma\n",
    "fused_proba_soma = (proba_ori_svm + proba_har_svm + proba_per_svm) / 3\n",
    "fused_pred_soma = np.argmax(fused_proba_soma, axis=1)\n",
    "fused_pred_soma += 1  # Ajustar a classe para começar de 1\n",
    "\n",
    "# Fusão das predições usando a regra do produto\n",
    "fused_proba_produto = proba_ori_svm * proba_har_svm * proba_per_svm\n",
    "fused_pred_produto = np.argmax(fused_proba_produto, axis=1)\n",
    "fused_pred_produto += 1  # Ajustar a classe para começar de 1\n",
    "\n",
    "# Avaliação das fusões\n",
    "def evaluate_predictions(y_true, y_pred, method_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    precision = precision_score(y_true, y_pred, average='weighted') * 100\n",
    "    recall = recall_score(y_true, y_pred, average='weighted') * 100\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted') * 100\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Função para realizar a avaliação com desvio padrão\n",
    "def evaluate_with_std(y_true, y_pred, method_name):\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "        accuracy, precision, recall, f1 = evaluate_predictions(y_true[i], y_pred[i], method_name)\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "    \n",
    "    mean_precision = np.mean(precisions)\n",
    "    std_precision = np.std(precisions)\n",
    "    \n",
    "    mean_recall = np.mean(recalls)\n",
    "    std_recall = np.std(recalls)\n",
    "    \n",
    "    mean_f1_score = np.mean(f1_scores)\n",
    "    std_f1_score = np.std(f1_scores)\n",
    "\n",
    "    print(f'{method_name} - Acurácia: {mean_accuracy:.2f}% ± {std_accuracy:.2f}%')\n",
    "    print(f'{method_name} - Precisão: {mean_precision:.2f}% ± {std_precision:.2f}%')\n",
    "    print(f'{method_name} - Recall: {mean_recall:.2f}% ± {std_recall:.2f}%')\n",
    "    print(f'{method_name} - F1-score: {mean_f1_score:.2f}% ± {std_f1_score:.2f}%\\n')\n",
    "\n",
    "# Avaliar as predições usando a regra da soma\n",
    "evaluate_with_std([y_test], [fused_pred_soma], \"Regra da Soma\")\n",
    "\n",
    "print('----------------------------------------------------')\n",
    "\n",
    "# Avaliar as predições usando a regra do produto\n",
    "evaluate_with_std([y_test], [fused_pred_produto], \"Regra do Produto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regra da Soma - Acurácia: 69.11%\n",
      "Regra da Soma - Precisão: 69.36%\n",
      "Regra da Soma - Recall: 69.11%\n",
      "Regra da Soma - F1-score: 68.31%\n",
      "----------------------------------------------------\n",
      "Regra do Produto - Acurácia: 70.00%\n",
      "Regra do Produto - Precisão: 70.85%\n",
      "Regra do Produto - Recall: 70.00%\n",
      "Regra do Produto - F1-score: 69.54%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Função para realizar predições com probabilidades usando KNN\n",
    "def kfoldcv_knn_proba(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    all_probas = []\n",
    "    y_tests = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "\n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "\n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Definir a grade de parâmetros para o Grid Search do KNN\n",
    "        param_grid = {\n",
    "            'n_neighbors': [3, 5, 7, 9, 11],\n",
    "            'metric': ['canberra', 'minkowski', 'euclidean', 'manhattan']\n",
    "        }\n",
    "\n",
    "        # Treinar o modelo KNN usando Grid Search\n",
    "        knn_model = KNeighborsClassifier()\n",
    "        grid_search = GridSearchCV(knn_model, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_knn = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões de probabilidades\n",
    "        y_proba = best_knn.predict_proba(X_test)\n",
    "        all_probas.append(y_proba)\n",
    "        y_tests.append(y_test)\n",
    "\n",
    "    return np.concatenate(all_probas), np.concatenate(y_tests)\n",
    "\n",
    "# Realizar a validação cruzada com probabilidades para cada conjunto de dados\n",
    "proba_ori_knn, y_test = kfoldcv_knn_proba(hist_ori, tags_ori, 3)\n",
    "proba_har_knn, _ = kfoldcv_knn_proba(hist_har, tags_har, 3)\n",
    "proba_per_knn, _ = kfoldcv_knn_proba(hist_per, tags_per, 3)\n",
    "\n",
    "# Fusão das predições usando a regra da soma\n",
    "fused_proba_soma = (proba_ori_knn + proba_har_knn + proba_per_knn) / 3\n",
    "fused_pred_soma = np.argmax(fused_proba_soma, axis=1)\n",
    "\n",
    "for i in range(0, len(fused_pred_soma)):\n",
    "    fused_pred_soma[i] = fused_pred_soma[i] + 1\n",
    "\n",
    "# Fusão das predições usando a regra do produto\n",
    "fused_proba_produto = proba_ori_knn * proba_har_knn * proba_per_knn\n",
    "fused_pred_produto = np.argmax(fused_proba_produto, axis=1)\n",
    "\n",
    "for i in range(0, len(fused_pred_produto)):\n",
    "    fused_pred_produto[i] = fused_pred_produto[i] + 1\n",
    "\n",
    "# Função para avaliar as predições\n",
    "def evaluate_predictions(y_true, y_pred, method_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    precision = precision_score(y_true, y_pred, average='weighted') * 100\n",
    "    recall = recall_score(y_true, y_pred, average='weighted') * 100\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted') * 100\n",
    "\n",
    "    print(f'{method_name} - Acurácia: {accuracy:.2f}%')\n",
    "    print(f'{method_name} - Precisão: {precision:.2f}%')\n",
    "    print(f'{method_name} - Recall: {recall:.2f}%')\n",
    "    print(f'{method_name} - F1-score: {f1:.2f}%')\n",
    "\n",
    "# Avaliar as predições usando a regra da soma\n",
    "evaluate_predictions(y_test, fused_pred_soma, \"Regra da Soma\")\n",
    "\n",
    "print('----------------------------------------------------')\n",
    "\n",
    "# Avaliar as predições usando a regra do produto\n",
    "evaluate_predictions(y_test, fused_pred_produto, \"Regra do Produto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regra da Soma - Acurácia: 79.00%\n",
      "Regra da Soma - Precisão: 79.06%\n",
      "Regra da Soma - Recall: 79.00%\n",
      "Regra da Soma - F1-score: 78.82%\n",
      "----------------------------------------------------\n",
      "Regra do Produto - Acurácia: 79.44%\n",
      "Regra do Produto - Precisão: 79.59%\n",
      "Regra do Produto - Recall: 79.44%\n",
      "Regra do Produto - F1-score: 79.32%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Função para combinar probabilidades de KNN e SVM\n",
    "def combine_probabilities(proba_knn, proba_svm):\n",
    "    # Combina as probabilidades usando uma média simples\n",
    "    combined_proba = (proba_knn + proba_svm) / 2\n",
    "    return combined_proba\n",
    "\n",
    "# Fusão das probabilidades dos modelos KNN e SVM\n",
    "proba_ori_combined = combine_probabilities(proba_ori_knn, proba_ori_svm)\n",
    "proba_har_combined = combine_probabilities(proba_har_knn, proba_har_svm)\n",
    "proba_per_combined = combine_probabilities(proba_per_knn, proba_per_svm)\n",
    "\n",
    "# Fusão das predições usando a regra da soma\n",
    "fused_proba_soma = (proba_ori_combined + proba_har_combined + proba_per_combined) / 3\n",
    "fused_pred_soma = np.argmax(fused_proba_soma, axis=1)\n",
    "\n",
    "for i in range(0, len(fused_pred_soma)):\n",
    "    fused_pred_soma[i] = fused_pred_soma[i] + 1\n",
    "\n",
    "# Fusão das predições usando a regra do produto\n",
    "fused_proba_produto = proba_ori_combined * proba_har_combined * proba_per_combined\n",
    "fused_pred_produto = np.argmax(fused_proba_produto, axis=1)\n",
    "\n",
    "for i in range(0, len(fused_pred_produto)):\n",
    "    fused_pred_produto[i] = fused_pred_produto[i] + 1\n",
    "\n",
    "# Função para avaliar as predições\n",
    "def evaluate_predictions(y_true, y_pred, method_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    precision = precision_score(y_true, y_pred, average='weighted') * 100\n",
    "    recall = recall_score(y_true, y_pred, average='weighted') * 100\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted') * 100\n",
    "\n",
    "    print(f'{method_name} - Acurácia: {accuracy:.2f}%')\n",
    "    print(f'{method_name} - Precisão: {precision:.2f}%')\n",
    "    print(f'{method_name} - Recall: {recall:.2f}%')\n",
    "    print(f'{method_name} - F1-score: {f1:.2f}%')\n",
    "\n",
    "# Avaliar as predições usando a regra da soma\n",
    "evaluate_predictions(y_test, fused_pred_soma, \"Regra da Soma\")\n",
    "\n",
    "print('----------------------------------------------------')\n",
    "\n",
    "# Avaliar as predições usando a regra do produto\n",
    "evaluate_predictions(y_test, fused_pred_produto, \"Regra do Produto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Função para realizar a validação cruzada\n",
    "def kfoldcv(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Configurar os parâmetros para o GridSearchCV (SVM)\n",
    "        param_grid = {\n",
    "            'C': [0.289],\n",
    "            'gamma': [1],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "\n",
    "        svm_model = SVC(random_state=42)\n",
    "        grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões e calcular as métricas\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        precisions.append(precision_score(y_test, y_pred, average='weighted'))\n",
    "        recalls.append(recall_score(y_test, y_pred, average='weighted'))\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies) * 100  # Converter para porcentagem\n",
    "    std_accuracy = np.std(accuracies) * 100  # Converter para porcentagem\n",
    "    \n",
    "    mean_precision = np.mean(precisions) * 100\n",
    "    std_precision = np.std(precisions) * 100\n",
    "    \n",
    "    mean_recall = np.mean(recalls) * 100\n",
    "    std_recall = np.std(recalls) * 100\n",
    "    \n",
    "    mean_f1_score = np.mean(f1_scores) * 100\n",
    "    std_f1_score = np.std(f1_scores) * 100\n",
    "\n",
    "    return (mean_accuracy, std_accuracy, \n",
    "            mean_precision, std_precision, \n",
    "            mean_recall, std_recall, \n",
    "            mean_f1_score, std_f1_score)\n",
    "\n",
    "# Realizar a validação cruzada\n",
    "(mean_accuracy, std_accuracy, \n",
    " mean_precision, std_precision, \n",
    " mean_recall, std_recall, \n",
    " mean_f1_score, std_f1_score) = kfoldcv(hist, tags, 3)\n",
    "\n",
    "print(f'Acurácia média: {mean_accuracy:.2f}% (Desvio padrão: {std_accuracy:.2f}%)')\n",
    "print(f'Precisão média: {mean_precision:.2f}% (Desvio padrão: {std_precision:.2f}%)')\n",
    "print(f'Recall médio: {mean_recall:.2f}% (Desvio padrão: {std_recall:.2f}%)')\n",
    "print(f'F1-score médio: {mean_f1_score:.2f}% (Desvio padrão: {std_f1_score:.2f}%)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Abrir o arquivo para escrita\n",
    "arquivo = open('resultados.txt', 'w')\n",
    "\n",
    "# Função para salvar as probabilidades em um formato legível\n",
    "def save_probabilities(arquivo, probas, label):\n",
    "    arquivo.write(f\"{label}:\\n\")\n",
    "    cont = 1\n",
    "    for i, prob in enumerate(probas):\n",
    "        prob_str = \", \".join([f\"{p:.4f}\" for p in prob])\n",
    "        arquivo.write(f\"tag {cont}: {prob_str}\\n\")\n",
    "        \n",
    "        if (i + 1) % 30 == 0:  # Incrementa o contador a cada 30 iterações\n",
    "            cont += 1\n",
    "        if i == 299:  # Reinicia o contador após 300 iterações\n",
    "            cont = 1\n",
    "    arquivo.write(\"\\n\")\n",
    "\n",
    "# Salvar as probabilidades\n",
    "save_probabilities(arquivo, proba_ori, \"Probabilidades Originais\")\n",
    "save_probabilities(arquivo, proba_har, \"Probabilidades Harmônicas\")\n",
    "save_probabilities(arquivo, proba_per, \"Probabilidades Percussivas\")\n",
    "\n",
    "# Fechar o arquivo\n",
    "arquivo.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
