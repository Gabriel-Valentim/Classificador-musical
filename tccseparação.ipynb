{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def calcula_lbp(imagem, P, R, metodo):\n",
    "    # Calcular o LBP\n",
    "    lbp = local_binary_pattern(imagem, P, R, metodo)\n",
    "    # Calcular o histograma LBP\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    hist, bins = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "    return hist\n",
    "\n",
    "def calcula_lbp_por_partes(imagem, P, R, metodo, num_partes=3):\n",
    "    altura = imagem.shape[0]  # Altura da imagem\n",
    "    largura = imagem.shape[1]  # Largura da imagem\n",
    "    \n",
    "    # Dividir a imagem horizontalmente em `num_partes` partes\n",
    "    parte_altura = altura // num_partes\n",
    "    \n",
    "    histogramas = []\n",
    "    \n",
    "    for i in range(num_partes):\n",
    "        # Definir a parte da imagem\n",
    "        inicio = i * parte_altura\n",
    "        fim = (i + 1) * parte_altura if i < num_partes - 1 else altura  # Garantir que a última parte pegue até o fim da imagem\n",
    "        \n",
    "        parte_imagem = imagem[inicio:fim, :]\n",
    "        \n",
    "        # Calcular o LBP e o histograma para a parte\n",
    "        hist = calcula_lbp(parte_imagem, P, R, metodo)\n",
    "        histogramas.append(hist)\n",
    "    \n",
    "    # Retornar os histogramas de cada parte separadamente\n",
    "    return histogramas[0], histogramas[1], histogramas[2]\n",
    "\n",
    "# Parâmetros LBP\n",
    "P = 8  # Número de pontos vizinhos\n",
    "R = 2  # Raio\n",
    "metodo = 'nri_uniform'\n",
    "\n",
    "# Diretórios\n",
    "diretorios = ['base/fold1', 'base/fold2', 'base/fold3']\n",
    "\n",
    "# Listas para armazenar os histogramas de cada parte\n",
    "hist_topo = []\n",
    "hist_meio = []\n",
    "hist_base = []\n",
    "tags = []\n",
    "\n",
    "# Carregar as imagens e calcular os histogramas LBP\n",
    "for i, diretorio in enumerate(diretorios):\n",
    "    lista_arquivos = os.listdir(diretorio)\n",
    "    for j, arquivo_img in enumerate(lista_arquivos):\n",
    "        caminho_imagem = os.path.join(diretorio, arquivo_img)\n",
    "        imagem = cv2.imread(caminho_imagem, cv2.IMREAD_GRAYSCALE)\n",
    "        if imagem is not None:\n",
    "            # Calcular os histogramas para as três partes (topo, meio, base)\n",
    "            hist_top, hist_mid, hist_bot = calcula_lbp_por_partes(imagem, P, R, metodo, num_partes=3)\n",
    "            hist_topo.append(hist_top)\n",
    "            hist_meio.append(hist_mid)\n",
    "            hist_base.append(hist_bot)\n",
    "            tags.append(j // 30 + 1)  # Tag para cada imagem\n",
    "\n",
    "# Converter as listas para arrays numpy\n",
    "hist_topo = np.array(hist_topo)\n",
    "hist_meio = np.array(hist_meio)\n",
    "hist_base = np.array(hist_base)\n",
    "tags_ori = np.array(tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regra da Soma - Acurácia: 81.33%\n",
      "Regra da Soma - Precisão: 81.53%\n",
      "Regra da Soma - Recall: 81.33%\n",
      "Regra da Soma - F1-score: 81.32%\n",
      "----------------------------------------------------\n",
      "Regra do Produto - Acurácia: 82.00%\n",
      "Regra do Produto - Precisão: 82.12%\n",
      "Regra do Produto - Recall: 82.00%\n",
      "Regra do Produto - F1-score: 81.98%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Função para realizar predições com probabilidades\n",
    "def kfoldcv_proba(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    all_probas = []\n",
    "    y_tests = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "        \n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "        \n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Configurar os parâmetros para o GridSearchCV (SVM)\n",
    "        param_grid = {\n",
    "            'C': [0.289],\n",
    "            'gamma': [1],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "\n",
    "        svm_model = SVC(random_state=42, probability=True)\n",
    "        grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões de probabilidades\n",
    "        y_proba = best_model.predict_proba(X_test)\n",
    "        all_probas.append(y_proba)\n",
    "        y_tests.append(y_test)\n",
    "    \n",
    "    return np.concatenate(all_probas), np.concatenate(y_tests)\n",
    "\n",
    "# Realizar a validação cruzada com probabilidades para cada conjunto de dados\n",
    "proba_topo_svm, y_test = kfoldcv_proba(hist_topo, tags_ori, 3)\n",
    "proba_meio_svm, _ = kfoldcv_proba(hist_meio, tags_ori, 3)\n",
    "proba_base_svm, _ = kfoldcv_proba(hist_base, tags_ori, 3)\n",
    "\n",
    "# Fusão das predições usando a regra da soma\n",
    "fused_proba_soma = (proba_topo_svm + proba_meio_svm + proba_base_svm) / 3\n",
    "fused_pred_soma = np.argmax(fused_proba_soma, axis=1)\n",
    "\n",
    "for i in range(0, len(fused_pred_soma)):\n",
    "    fused_pred_soma[i] = fused_pred_soma[i] + 1\n",
    "\n",
    "# Fusão das predições usando a regra do produto\n",
    "fused_proba_produto = proba_topo_svm * proba_meio_svm * proba_base_svm\n",
    "fused_pred_produto = np.argmax(fused_proba_produto, axis=1)\n",
    "\n",
    "for i in range(0, len(fused_pred_produto)):\n",
    "    fused_pred_produto[i] = fused_pred_produto[i] + 1\n",
    "\n",
    "# Avaliação das fusões\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred, method_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    precision = precision_score(y_true, y_pred, average='weighted') * 100\n",
    "    recall = recall_score(y_true, y_pred, average='weighted') * 100\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted') * 100\n",
    "\n",
    "    print(f'{method_name} - Acurácia: {accuracy:.2f}%')\n",
    "    print(f'{method_name} - Precisão: {precision:.2f}%')\n",
    "    print(f'{method_name} - Recall: {recall:.2f}%')\n",
    "    print(f'{method_name} - F1-score: {f1:.2f}%')\n",
    "\n",
    "# Avaliar as predições usando a regra da soma\n",
    "evaluate_predictions(y_test, fused_pred_soma, \"Regra da Soma\")\n",
    "\n",
    "print('----------------------------------------------------')\n",
    "\n",
    "# Avaliar as predições usando a regra do produto\n",
    "evaluate_predictions(y_test, fused_pred_produto, \"Regra do Produto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regra da Soma - Acurácia: 70.67%\n",
      "Regra da Soma - Precisão: 70.88%\n",
      "Regra da Soma - Recall: 70.67%\n",
      "Regra da Soma - F1-score: 70.33%\n",
      "----------------------------------------------------\n",
      "Regra do Produto - Acurácia: 68.44%\n",
      "Regra do Produto - Precisão: 73.01%\n",
      "Regra do Produto - Recall: 68.44%\n",
      "Regra do Produto - F1-score: 69.19%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Função para realizar predições com probabilidades usando KNN\n",
    "def kfoldcv_knn_proba(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    all_probas = []\n",
    "    y_tests = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "\n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "\n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Definir a grade de parâmetros para o Grid Search do KNN\n",
    "        param_grid = {\n",
    "            'n_neighbors': [3, 5, 7, 9, 11],\n",
    "            'metric': ['canberra', 'minkowski', 'euclidean', 'manhattan']\n",
    "        }\n",
    "\n",
    "        # Treinar o modelo KNN usando Grid Search\n",
    "        knn_model = KNeighborsClassifier()\n",
    "        grid_search = GridSearchCV(knn_model, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Melhor modelo encontrado pelo Grid Search\n",
    "        best_knn = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões de probabilidades\n",
    "        y_proba = best_knn.predict_proba(X_test)\n",
    "        all_probas.append(y_proba)\n",
    "        y_tests.append(y_test)\n",
    "\n",
    "    return np.concatenate(all_probas), np.concatenate(y_tests)\n",
    "\n",
    "# Realizar a validação cruzada com probabilidades para cada conjunto de dados\n",
    "proba_topo_knn, y_test = kfoldcv_knn_proba(hist_topo, tags_ori, 3)\n",
    "proba_meio_knn, _ = kfoldcv_knn_proba(hist_meio, tags_ori, 3)\n",
    "proba_base_knn, _ = kfoldcv_knn_proba(hist_base, tags_ori, 3)\n",
    "\n",
    "# Fusão das predições usando a regra da soma\n",
    "fused_proba_soma = (proba_topo_knn + proba_meio_knn + proba_base_knn) / 3\n",
    "fused_pred_soma = np.argmax(fused_proba_soma, axis=1)\n",
    "\n",
    "for i in range(0, len(fused_pred_soma)):\n",
    "    fused_pred_soma[i] = fused_pred_soma[i] + 1\n",
    "\n",
    "# Fusão das predições usando a regra do produto\n",
    "fused_proba_produto = proba_topo_knn * proba_meio_knn * proba_base_knn\n",
    "fused_pred_produto = np.argmax(fused_proba_produto, axis=1)\n",
    "\n",
    "for i in range(0, len(fused_pred_produto)):\n",
    "    fused_pred_produto[i] = fused_pred_produto[i] + 1\n",
    "\n",
    "# Função para avaliar as predições\n",
    "def evaluate_predictions(y_true, y_pred, method_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    precision = precision_score(y_true, y_pred, average='weighted') * 100\n",
    "    recall = recall_score(y_true, y_pred, average='weighted') * 100\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted') * 100\n",
    "\n",
    "    print(f'{method_name} - Acurácia: {accuracy:.2f}%')\n",
    "    print(f'{method_name} - Precisão: {precision:.2f}%')\n",
    "    print(f'{method_name} - Recall: {recall:.2f}%')\n",
    "    print(f'{method_name} - F1-score: {f1:.2f}%')\n",
    "\n",
    "# Avaliar as predições usando a regra da soma\n",
    "evaluate_predictions(y_test, fused_pred_soma, \"Regra da Soma\")\n",
    "\n",
    "print('----------------------------------------------------')\n",
    "\n",
    "# Avaliar as predições usando a regra do produto\n",
    "evaluate_predictions(y_test, fused_pred_produto, \"Regra do Produto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regra da Soma - Acurácia: 73.33%\n",
      "Regra da Soma - Precisão: 73.38%\n",
      "Regra da Soma - Recall: 73.33%\n",
      "Regra da Soma - F1-score: 72.96%\n",
      "----------------------------------------------------\n",
      "Regra do Produto - Acurácia: 74.33%\n",
      "Regra do Produto - Precisão: 74.38%\n",
      "Regra do Produto - Recall: 74.33%\n",
      "Regra do Produto - F1-score: 74.13%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Função para realizar predições com probabilidades usando RandomForest\n",
    "def kfoldcv_rf_proba(hist, tags, k):\n",
    "    fold_size = len(hist) // k\n",
    "    all_probas = []\n",
    "    y_tests = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Define os índices de teste e treino baseados na ordem\n",
    "        test_idx = np.arange(i*fold_size, (i+1)*fold_size)\n",
    "        train_idx = np.concatenate([np.arange(0, i*fold_size), np.arange((i+1)*fold_size, len(hist))])\n",
    "\n",
    "        X_train, X_test = hist[train_idx], hist[test_idx]\n",
    "        y_train, y_test = tags[train_idx], tags[test_idx]\n",
    "\n",
    "        # Padronizar os dados\n",
    "        scaler = StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Treinar o modelo Random Forest\n",
    "        rf_model = RandomForestClassifier(n_estimators=4000, random_state=0, n_jobs=-1)\n",
    "        rf_model.fit(X_train, y_train)\n",
    "\n",
    "        # Fazer previsões de probabilidades\n",
    "        y_proba = rf_model.predict_proba(X_test)\n",
    "        all_probas.append(y_proba)\n",
    "        y_tests.append(y_test)\n",
    "\n",
    "    return np.concatenate(all_probas), np.concatenate(y_tests)\n",
    "\n",
    "# Realizar a validação cruzada com probabilidades para cada conjunto de dados\n",
    "proba_topo_rf, y_test = kfoldcv_rf_proba(hist_topo, tags_ori, 3)\n",
    "proba_meio_rf, _ = kfoldcv_rf_proba(hist_meio, tags_ori, 3)\n",
    "proba_base_rf, _ = kfoldcv_rf_proba(hist_base, tags_ori, 3)\n",
    "\n",
    "# Fusão das predições usando a regra da soma\n",
    "fused_proba_soma = (proba_topo_rf + proba_meio_rf + proba_base_rf) / 3\n",
    "fused_pred_soma = np.argmax(fused_proba_soma, axis=1)\n",
    "\n",
    "for i in range(0, len(fused_pred_soma)):\n",
    "    fused_pred_soma[i] = fused_pred_soma[i] + 1\n",
    "\n",
    "# Fusão das predições usando a regra do produto\n",
    "fused_proba_produto = proba_topo_rf * proba_meio_rf * proba_base_rf\n",
    "fused_pred_produto = np.argmax(fused_proba_produto, axis=1)\n",
    "\n",
    "for i in range(0, len(fused_pred_produto)):\n",
    "    fused_pred_produto[i] = fused_pred_produto[i] + 1\n",
    "\n",
    "# Função para avaliar as predições\n",
    "def evaluate_predictions(y_true, y_pred, method_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    precision = precision_score(y_true, y_pred, average='weighted') * 100\n",
    "    recall = recall_score(y_true, y_pred, average='weighted') * 100\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted') * 100\n",
    "\n",
    "    print(f'{method_name} - Acurácia: {accuracy:.2f}%')\n",
    "    print(f'{method_name} - Precisão: {precision:.2f}%')\n",
    "    print(f'{method_name} - Recall: {recall:.2f}%')\n",
    "    print(f'{method_name} - F1-score: {f1:.2f}%')\n",
    "\n",
    "# Avaliar as predições usando a regra da soma\n",
    "evaluate_predictions(y_test, fused_pred_soma, \"Regra da Soma\")\n",
    "\n",
    "print('----------------------------------------------------')\n",
    "\n",
    "# Avaliar as predições usando a regra do produto\n",
    "evaluate_predictions(y_test, fused_pred_produto, \"Regra do Produto\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dividida em 10 partes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (900,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m             tags\u001b[38;5;241m.\u001b[39mappend(j \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m30\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Tag para cada imagem\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Converter as listas para arrays numpy\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m hist_partes \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(hist) \u001b[38;5;28;01mfor\u001b[39;00m hist \u001b[38;5;129;01min\u001b[39;00m hist_partes]\n\u001b[0;32m     64\u001b[0m tags_ori \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(tags)\n",
      "Cell \u001b[1;32mIn[5], line 63\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     60\u001b[0m             tags\u001b[38;5;241m.\u001b[39mappend(j \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m30\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Tag para cada imagem\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Converter as listas para arrays numpy\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m hist_partes \u001b[38;5;241m=\u001b[39m [\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhist\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m hist \u001b[38;5;129;01min\u001b[39;00m hist_partes]\n\u001b[0;32m     64\u001b[0m tags_ori \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(tags)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (900,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def calcula_lbp(imagem, P, R, metodo):\n",
    "    # Calcular o LBP\n",
    "    lbp = local_binary_pattern(imagem, P, R, metodo)\n",
    "    # Calcular o histograma LBP\n",
    "    n_bins = int(lbp.max() + 1)\n",
    "    hist, bins = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
    "    return hist\n",
    "\n",
    "def calcula_lbp_por_partes(imagem, P, R, metodo, num_partes=10):\n",
    "    altura = imagem.shape[0]  # Altura da imagem\n",
    "    largura = imagem.shape[1]  # Largura da imagem\n",
    "    \n",
    "    # Dividir a imagem horizontalmente em `num_partes` partes\n",
    "    parte_altura = altura // num_partes\n",
    "    \n",
    "    histogramas = []\n",
    "    \n",
    "    for i in range(num_partes):\n",
    "        # Definir a parte da imagem\n",
    "        inicio = i * parte_altura\n",
    "        fim = (i + 1) * parte_altura if i < num_partes - 1 else altura  # Garantir que a última parte pegue até o fim da imagem\n",
    "        \n",
    "        parte_imagem = imagem[inicio:fim, :]\n",
    "        \n",
    "        # Calcular o LBP e o histograma para a parte\n",
    "        hist = calcula_lbp(parte_imagem, P, R, metodo)\n",
    "        histogramas.append(hist)\n",
    "    \n",
    "    # Retornar os histogramas de cada parte separadamente (agora são 10)\n",
    "    return histogramas\n",
    "\n",
    "# Parâmetros LBP\n",
    "P = 8  # Número de pontos vizinhos\n",
    "R = 2  # Raio\n",
    "metodo = 'nri_uniform'\n",
    "\n",
    "# Diretórios\n",
    "diretorios = ['base/fold1', 'base/fold2', 'base/fold3']\n",
    "\n",
    "# Lista para armazenar os histogramas de cada parte\n",
    "hist_partes = [[] for _ in range(10)]\n",
    "tags = []\n",
    "\n",
    "# Carregar as imagens e calcular os histogramas LBP\n",
    "for i, diretorio in enumerate(diretorios):\n",
    "    lista_arquivos = os.listdir(diretorio)\n",
    "    for j, arquivo_img in enumerate(lista_arquivos):\n",
    "        caminho_imagem = os.path.join(diretorio, arquivo_img)\n",
    "        imagem = cv2.imread(caminho_imagem, cv2.IMREAD_GRAYSCALE)\n",
    "        if imagem is not None:\n",
    "            # Calcular os histogramas para as 10 partes\n",
    "            histogramas = calcula_lbp_por_partes(imagem, P, R, metodo, num_partes=10)\n",
    "            for idx, hist in enumerate(histogramas):\n",
    "                hist_partes[idx].append(hist)\n",
    "            tags.append(j // 30 + 1)  # Tag para cada imagem\n",
    "\n",
    "# Converter as listas para arrays numpy\n",
    "hist_partes = [np.array(hist) for hist in hist_partes]\n",
    "tags_ori = np.array(tags)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
